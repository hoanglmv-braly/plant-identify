{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from typing import Optional\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, root_dir='/Users/braly/Desktop/lmvh/plant-identify/dataset',\n",
    "                 split='train',\n",
    "                 transform=None,\n",
    "                 extensions=('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): đường dẫn tới thư mục chứa train/val/test\n",
    "            split (str): 'train' | 'val' | 'test'\n",
    "            transform (callable, optional): torchvision transforms or any callable applied lên PIL image\n",
    "            extensions (tuple): các hậu tố file ảnh chấp nhận\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.extensions = tuple(e.lower() for e in extensions)\n",
    "\n",
    "        self.images = []   # danh sách đường dẫn ảnh\n",
    "        self.labels = []   # label dưới dạng index\n",
    "        self.classes = []  # tên lớp (sorted)\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "\n",
    "        split_dir = os.path.join(self.root_dir, self.split)\n",
    "        if not os.path.isdir(split_dir):\n",
    "            raise ValueError(f\"Split folder not found: {split_dir}\")\n",
    "\n",
    "        # Lấy danh sách lớp (thư mục con) và map sang index\n",
    "        classes = [d for d in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, d))]\n",
    "        classes = sorted(classes)\n",
    "        if len(classes) == 0:\n",
    "            raise ValueError(f\"No class subfolders found in {split_dir}\")\n",
    "\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
    "\n",
    "        # Duyệt từng thư mục lớp và thu thập ảnh\n",
    "        for cls_name in self.classes:\n",
    "            cls_dir = os.path.join(split_dir, cls_name)\n",
    "            for root, _, files in os.walk(cls_dir):\n",
    "                for fname in files:\n",
    "                    if fname.lower().endswith(self.extensions):\n",
    "                        path = os.path.join(root, fname)\n",
    "                        self.images.append(path)\n",
    "                        self.labels.append(self.class_to_idx[cls_name])\n",
    "\n",
    "        if len(self.images) == 0:\n",
    "            raise ValueError(f\"No images found in {split_dir} with extensions {self.extensions}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Trả về: (image, label_index)\n",
    "        - image: PIL.Image (nếu transform None) hoặc transform(image)\n",
    "        - label_index: int (index của lớp)\n",
    "        \"\"\"\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Mở ảnh an toàn\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    # Tiện ích: trả về số ảnh / lớp\n",
    "    def get_class_counts(self):\n",
    "        \"\"\"Trả về dict: {class_name: count}\"\"\"\n",
    "        counts = Counter()\n",
    "        for lbl in self.labels:\n",
    "            counts[self.idx_to_class[lbl]] += 1\n",
    "        return dict(counts)\n",
    "\n",
    "    def print_stats(self):\n",
    "        \"\"\"In thông tin tóm tắt dataset\"\"\"\n",
    "        total = len(self)\n",
    "        counts = self.get_class_counts()\n",
    "        print(f\"Dataset split: {self.split}\")\n",
    "        print(f\"Root dir: {self.root_dir}\")\n",
    "        print(f\"Total images: {total}\")\n",
    "        print(\"Number of classes:\", len(self.classes))\n",
    "        print(\"Class -> index mapping:\")\n",
    "        for cls, idx in self.class_to_idx.items():\n",
    "            print(f\"  {cls:20s} -> {idx:3d} ({counts.get(cls,0)} images)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47cadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6426d464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: train\n",
      "Root dir: /Users/braly/Desktop/lmvh/plant-identify/dataset\n",
      "Total images: 20684\n",
      "Number of classes: 47\n",
      "Class -> index mapping:\n",
      "  African Violet (Saintpaulia ionantha) ->   0 (478 images)\n",
      "  Aloe Vera            ->   1 (366 images)\n",
      "  Anthurium (Anthurium andraeanum) ->   2 (644 images)\n",
      "  Areca Palm (Dypsis lutescens) ->   3 (258 images)\n",
      "  Asparagus Fern (Asparagus setaceus) ->   4 (218 images)\n",
      "  Begonia (Begonia spp.) ->   5 (312 images)\n",
      "  Bird of Paradise (Strelitzia reginae) ->   6 (254 images)\n",
      "  Birds Nest Fern (Asplenium nidus) ->   7 (402 images)\n",
      "  Boston Fern (Nephrolepis exaltata) ->   8 (420 images)\n",
      "  Calathea             ->   9 (448 images)\n",
      "  Cast Iron Plant (Aspidistra elatior) ->  10 (366 images)\n",
      "  Chinese Money Plant (Pilea peperomioides) ->  11 (530 images)\n",
      "  Chinese evergreen (Aglaonema) ->  12 (734 images)\n",
      "  Christmas Cactus (Schlumbergera bridgesii) ->  13 (418 images)\n",
      "  Chrysanthemum        ->  14 (288 images)\n",
      "  Ctenanthe            ->  15 (510 images)\n",
      "  Daffodils (Narcissus spp.) ->  16 (594 images)\n",
      "  Dracaena             ->  17 (362 images)\n",
      "  Dumb Cane (Dieffenbachia spp.) ->  18 (760 images)\n",
      "  Elephant Ear (Alocasia spp.) ->  19 (470 images)\n",
      "  English Ivy (Hedera helix) ->  20 (358 images)\n",
      "  Hyacinth (Hyacinthus orientalis) ->  21 (438 images)\n",
      "  Iron Cross begonia (Begonia masoniana) ->  22 (402 images)\n",
      "  Jade plant (Crassula ovata) ->  23 (522 images)\n",
      "  Kalanchoe            ->  24 (170 images)\n",
      "  Lilium (Hemerocallis) ->  25 (674 images)\n",
      "  Lily of the valley (Convallaria majalis) ->  26 (564 images)\n",
      "  Money Tree (Pachira aquatica) ->  27 (504 images)\n",
      "  Monstera Deliciosa (Monstera deliciosa) ->  28 (758 images)\n",
      "  Orchid               ->  29 (312 images)\n",
      "  Parlor Palm (Chamaedorea elegans) ->  30 (474 images)\n",
      "  Peace lily           ->  31 (526 images)\n",
      "  Poinsettia (Euphorbia pulcherrima) ->  32 (436 images)\n",
      "  Polka Dot Plant (Hypoestes phyllostachya) ->  33 (484 images)\n",
      "  Ponytail Palm (Beaucarnea recurvata) ->  34 (250 images)\n",
      "  Pothos (Ivy arum)    ->  35 (342 images)\n",
      "  Prayer Plant (Maranta leuconeura) ->  36 (574 images)\n",
      "  Rattlesnake Plant (Calathea lancifolia) ->  37 (460 images)\n",
      "  Rubber Plant (Ficus elastica) ->  38 (384 images)\n",
      "  Sago Palm (Cycas revoluta) ->  39 (262 images)\n",
      "  Schefflera           ->  40 (466 images)\n",
      "  Snake plant (Sanseviera) ->  41 (532 images)\n",
      "  Tradescantia         ->  42 (476 images)\n",
      "  Tulip                ->  43 (466 images)\n",
      "  Venus Flytrap        ->  44 (298 images)\n",
      "  Yucca                ->  45 (98 images)\n",
      "  ZZ Plant (Zamioculcas zamiifolia) ->  46 (622 images)\n",
      "Total train images: 20684\n",
      "<class 'torch.Tensor'> 0\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root = '/Users/braly/Desktop/lmvh/plant-identify/dataset'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = PlantDataset(root_dir=root, split='train', transform=transform)\n",
    "val_ds   = PlantDataset(root_dir=root, split='valid', transform=transform)\n",
    "test_ds  = PlantDataset(root_dir=root, split='test', transform=transform)\n",
    "\n",
    "train_ds.print_stats()\n",
    "print(\"Total train images:\", len(train_ds))\n",
    "\n",
    "# Lấy 1 mẫu\n",
    "img, label = train_ds[0]\n",
    "print(type(img), label)  # img là Tensor nếu transform -> ToTensor, label là int (index)\n",
    "\n",
    "# Dùng DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "373b5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 root_dir: str,\n",
    "                 image_size: int = 224,\n",
    "                 batch_size: int = 32,\n",
    "                 num_workers: int = 4,\n",
    "                 pin_memory: bool = True):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "        # transforms\n",
    "        self.train_transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.val_transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # placeholders set in setup()\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        # Called on every GPU in DDP — keep idempotent\n",
    "        if stage in (None, 'fit'):\n",
    "            self.train_dataset = PlantDataset(self.root_dir, split='train', transform=self.train_transform)\n",
    "            self.val_dataset = PlantDataset(self.root_dir, split='val', transform=self.val_transform)\n",
    "            self.num_classes = len(self.train_dataset.classes)\n",
    "        if stage in (None, 'test'):\n",
    "            self.test_dataset = PlantDataset(self.root_dir, split='test', transform=self.val_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=self.num_workers,\n",
    "                          pin_memory=self.pin_memory)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers,\n",
    "                          pin_memory=self.pin_memory)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataset is None:\n",
    "            return None\n",
    "        return DataLoader(self.test_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers,\n",
    "                          pin_memory=self.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e94a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTLightning(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 num_classes: int,\n",
    "                 lr: float = 3e-4,\n",
    "                 weight_decay: float = 1e-2,\n",
    "                 backbone_name: str = \"vit_base_patch16_224\",\n",
    "                 pretrained: bool = True,\n",
    "                 freeze_backbone: bool = False):\n",
    "        \"\"\"\n",
    "        If timm is available, use timm.create_model(backbone_name, pretrained=True, num_classes=num_classes).\n",
    "        Otherwise try torchvision's vit_b_16 (if installed).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.backbone_name = backbone_name\n",
    "        self.pretrained = pretrained\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "\n",
    "        # Build model\n",
    "        if _USE_TIMM:\n",
    "            # timm handles classifier creation\n",
    "            self.model = timm.create_model(self.backbone_name, pretrained=self.pretrained, num_classes=self.num_classes)\n",
    "        else:\n",
    "            # fallback to torchvision ViT if available\n",
    "            try:\n",
    "                from torchvision import models as tv_models\n",
    "                vit_builder = getattr(tv_models, \"vit_b_16\", None)\n",
    "                if vit_builder is None:\n",
    "                    raise RuntimeError(\"torchvision ViT not available; please install timm.\")\n",
    "                # torchvision vit builder signatures vary; try to create without classifier then add head\n",
    "                backbone = vit_builder(weights=\"IMAGENET1K_V1\") if hasattr(vit_builder, '__call__') else vit_builder(pretrained=self.pretrained)\n",
    "                # remove existing head if present\n",
    "                if hasattr(backbone, 'heads'):\n",
    "                    feat_dim = backbone.heads.head.in_features if hasattr(backbone.heads, 'head') else getattr(backbone, 'hidden_dim', 768)\n",
    "                    backbone.heads = nn.Identity()\n",
    "                elif hasattr(backbone, 'head'):\n",
    "                    feat_dim = backbone.head.in_features\n",
    "                    backbone.head = nn.Identity()\n",
    "                else:\n",
    "                    feat_dim = getattr(backbone, 'hidden_dim', 768)\n",
    "                # create classifier\n",
    "                head = nn.Linear(feat_dim, self.num_classes)\n",
    "                self.model = nn.Sequential(backbone, head)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"No ViT backbone available. Install timm or use recent torchvision.\") from e\n",
    "\n",
    "        # optionally freeze backbone parameters for fine-tuning head only\n",
    "        if self.freeze_backbone:\n",
    "            for name, p in self.model.named_parameters():\n",
    "                if \"head\" not in name and \"heads\" not in name and \"classifier\" not in name:\n",
    "                    p.requires_grad = False\n",
    "\n",
    "        # loss + metrics\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # use torchmetrics for metrics if available\n",
    "        try:\n",
    "            import torchmetrics\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        except Exception:\n",
    "            # fallback simple trackers\n",
    "            self.train_acc = None\n",
    "            self.val_acc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # log loss\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=False)\n",
    "        if self.train_acc is not None:\n",
    "            acc = self.train_acc(preds, y)\n",
    "            self.log(\"train/acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        else:\n",
    "            # rough acc\n",
    "            acc = (preds == y).float().mean()\n",
    "            self.log(\"train/acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        if self.val_acc is not None:\n",
    "            acc = self.val_acc(preds, y)\n",
    "            self.log(\"val/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        else:\n",
    "            acc = (preds == y).float().mean()\n",
    "            self.log(\"val/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True)\n",
    "        if self.val_acc is not None:\n",
    "            acc = self.val_acc(preds, y)\n",
    "            self.log(\"test/acc\", acc, on_step=False, on_epoch=True)\n",
    "        else:\n",
    "            acc = (preds == y).float().mean()\n",
    "            self.log(\"test/acc\", acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        # small scheduler example (cosine)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val/loss\"}}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
