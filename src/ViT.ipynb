{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b30cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from typing import Optional\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6c570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (25.2)\n",
      "Requirement already satisfied: tensorboard in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (1.73.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (2.2.6)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (11.3.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (6.31.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "tensorboard found: True\n",
      "tensorboard version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# 1) Cài vào đúng Python của kernel hiện tại\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install tensorboard\n",
    "\n",
    "# 2) Kiểm tra cài thành công\n",
    "import importlib, pkgutil\n",
    "spec = importlib.util.find_spec(\"tensorboard\")\n",
    "print(\"tensorboard found:\", spec is not None)\n",
    "if spec:\n",
    "    import tensorboard\n",
    "    print(\"tensorboard version:\", tensorboard.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a671aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import timm\n",
    "    _USE_TIMM = True\n",
    "except ImportError:\n",
    "    _USE_TIMM = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29fe774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, root_dir='/Users/braly/Desktop/lmvh/plant-identify/dataset',\n",
    "                 split='train',\n",
    "                 transform=None,\n",
    "                 extensions=('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): đường dẫn tới thư mục chứa train/val/test\n",
    "            split (str): 'train' | 'val' | 'test'\n",
    "            transform (callable, optional): torchvision transforms or any callable applied lên PIL image\n",
    "            extensions (tuple): các hậu tố file ảnh chấp nhận\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.extensions = tuple(e.lower() for e in extensions)\n",
    "\n",
    "        self.images = []   # danh sách đường dẫn ảnh\n",
    "        self.labels = []   # label dưới dạng index\n",
    "        self.classes = []  # tên lớp (sorted)\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "\n",
    "        split_dir = os.path.join(self.root_dir, self.split)\n",
    "        if not os.path.isdir(split_dir):\n",
    "            raise ValueError(f\"Split folder not found: {split_dir}\")\n",
    "\n",
    "        # Lấy danh sách lớp (thư mục con) và map sang index\n",
    "        classes = [d for d in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, d))]\n",
    "        classes = sorted(classes)\n",
    "        if len(classes) == 0:\n",
    "            raise ValueError(f\"No class subfolders found in {split_dir}\")\n",
    "\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
    "\n",
    "        # Duyệt từng thư mục lớp và thu thập ảnh\n",
    "        for cls_name in self.classes:\n",
    "            cls_dir = os.path.join(split_dir, cls_name)\n",
    "            for root, _, files in os.walk(cls_dir):\n",
    "                for fname in files:\n",
    "                    if fname.lower().endswith(self.extensions):\n",
    "                        path = os.path.join(root, fname)\n",
    "                        self.images.append(path)\n",
    "                        self.labels.append(self.class_to_idx[cls_name])\n",
    "\n",
    "        if len(self.images) == 0:\n",
    "            raise ValueError(f\"No images found in {split_dir} with extensions {self.extensions}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Trả về: (image, label_index)\n",
    "        - image: PIL.Image (nếu transform None) hoặc transform(image)\n",
    "        - label_index: int (index của lớp)\n",
    "        \"\"\"\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Mở ảnh an toàn\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    # Tiện ích: trả về số ảnh / lớp\n",
    "    def get_class_counts(self):\n",
    "        \"\"\"Trả về dict: {class_name: count}\"\"\"\n",
    "        counts = Counter()\n",
    "        for lbl in self.labels:\n",
    "            counts[self.idx_to_class[lbl]] += 1\n",
    "        return dict(counts)\n",
    "\n",
    "    def print_stats(self):\n",
    "        \"\"\"In thông tin tóm tắt dataset\"\"\"\n",
    "        total = len(self)\n",
    "        counts = self.get_class_counts()\n",
    "        print(f\"Dataset split: {self.split}\")\n",
    "        print(f\"Root dir: {self.root_dir}\")\n",
    "        print(f\"Total images: {total}\")\n",
    "        print(\"Number of classes:\", len(self.classes))\n",
    "        print(\"Class -> index mapping:\")\n",
    "        for cls, idx in self.class_to_idx.items():\n",
    "            print(f\"  {cls:20s} -> {idx:3d} ({counts.get(cls,0)} images)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47cadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6426d464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: train\n",
      "Root dir: /Users/braly/Desktop/lmvh/plant-identify/dataset\n",
      "Total images: 20684\n",
      "Number of classes: 47\n",
      "Class -> index mapping:\n",
      "  African Violet (Saintpaulia ionantha) ->   0 (478 images)\n",
      "  Aloe Vera            ->   1 (366 images)\n",
      "  Anthurium (Anthurium andraeanum) ->   2 (644 images)\n",
      "  Areca Palm (Dypsis lutescens) ->   3 (258 images)\n",
      "  Asparagus Fern (Asparagus setaceus) ->   4 (218 images)\n",
      "  Begonia (Begonia spp.) ->   5 (312 images)\n",
      "  Bird of Paradise (Strelitzia reginae) ->   6 (254 images)\n",
      "  Birds Nest Fern (Asplenium nidus) ->   7 (402 images)\n",
      "  Boston Fern (Nephrolepis exaltata) ->   8 (420 images)\n",
      "  Calathea             ->   9 (448 images)\n",
      "  Cast Iron Plant (Aspidistra elatior) ->  10 (366 images)\n",
      "  Chinese Money Plant (Pilea peperomioides) ->  11 (530 images)\n",
      "  Chinese evergreen (Aglaonema) ->  12 (734 images)\n",
      "  Christmas Cactus (Schlumbergera bridgesii) ->  13 (418 images)\n",
      "  Chrysanthemum        ->  14 (288 images)\n",
      "  Ctenanthe            ->  15 (510 images)\n",
      "  Daffodils (Narcissus spp.) ->  16 (594 images)\n",
      "  Dracaena             ->  17 (362 images)\n",
      "  Dumb Cane (Dieffenbachia spp.) ->  18 (760 images)\n",
      "  Elephant Ear (Alocasia spp.) ->  19 (470 images)\n",
      "  English Ivy (Hedera helix) ->  20 (358 images)\n",
      "  Hyacinth (Hyacinthus orientalis) ->  21 (438 images)\n",
      "  Iron Cross begonia (Begonia masoniana) ->  22 (402 images)\n",
      "  Jade plant (Crassula ovata) ->  23 (522 images)\n",
      "  Kalanchoe            ->  24 (170 images)\n",
      "  Lilium (Hemerocallis) ->  25 (674 images)\n",
      "  Lily of the valley (Convallaria majalis) ->  26 (564 images)\n",
      "  Money Tree (Pachira aquatica) ->  27 (504 images)\n",
      "  Monstera Deliciosa (Monstera deliciosa) ->  28 (758 images)\n",
      "  Orchid               ->  29 (312 images)\n",
      "  Parlor Palm (Chamaedorea elegans) ->  30 (474 images)\n",
      "  Peace lily           ->  31 (526 images)\n",
      "  Poinsettia (Euphorbia pulcherrima) ->  32 (436 images)\n",
      "  Polka Dot Plant (Hypoestes phyllostachya) ->  33 (484 images)\n",
      "  Ponytail Palm (Beaucarnea recurvata) ->  34 (250 images)\n",
      "  Pothos (Ivy arum)    ->  35 (342 images)\n",
      "  Prayer Plant (Maranta leuconeura) ->  36 (574 images)\n",
      "  Rattlesnake Plant (Calathea lancifolia) ->  37 (460 images)\n",
      "  Rubber Plant (Ficus elastica) ->  38 (384 images)\n",
      "  Sago Palm (Cycas revoluta) ->  39 (262 images)\n",
      "  Schefflera           ->  40 (466 images)\n",
      "  Snake plant (Sanseviera) ->  41 (532 images)\n",
      "  Tradescantia         ->  42 (476 images)\n",
      "  Tulip                ->  43 (466 images)\n",
      "  Venus Flytrap        ->  44 (298 images)\n",
      "  Yucca                ->  45 (98 images)\n",
      "  ZZ Plant (Zamioculcas zamiifolia) ->  46 (622 images)\n",
      "Total train images: 20684\n",
      "<class 'torch.Tensor'> 0\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root = '/Users/braly/Desktop/lmvh/plant-identify/dataset'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = PlantDataset(root_dir=root, split='train', transform=transform)\n",
    "val_ds   = PlantDataset(root_dir=root, split='valid', transform=transform)\n",
    "test_ds  = PlantDataset(root_dir=root, split='test', transform=transform)\n",
    "\n",
    "train_ds.print_stats()\n",
    "print(\"Total train images:\", len(train_ds))\n",
    "\n",
    "# Lấy 1 mẫu\n",
    "img, label = train_ds[0]\n",
    "print(type(img), label)  # img là Tensor nếu transform -> ToTensor, label là int (index)\n",
    "\n",
    "# Dùng DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373b5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 root_dir: str,\n",
    "                 image_size: int = 224,\n",
    "                 batch_size: int = 32,\n",
    "                 num_workers: int = 4,\n",
    "                 pin_memory: bool = True):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "        # transforms\n",
    "        self.train_transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.val_transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # placeholders set in setup()\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        # Called on every GPU in DDP — keep idempotent\n",
    "        if stage in (None, 'fit'):\n",
    "            self.train_dataset = PlantDataset(self.root_dir, split='train', transform=self.train_transform)\n",
    "            self.val_dataset = PlantDataset(self.root_dir, split='valid', transform=self.val_transform)\n",
    "            self.num_classes = len(self.train_dataset.classes)\n",
    "        if stage in (None, 'test'):\n",
    "            self.test_dataset = PlantDataset(self.root_dir, split='test', transform=self.val_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=self.num_workers,\n",
    "                          pin_memory=self.pin_memory)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers,\n",
    "                          pin_memory=self.pin_memory)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataset is None:\n",
    "            return None\n",
    "        return DataLoader(self.test_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers,\n",
    "                          pin_memory=self.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e94a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTLightning(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 num_classes: int,\n",
    "                 lr: float = 3e-4,\n",
    "                 weight_decay: float = 1e-2,\n",
    "                 backbone_name: str = \"vit_base_patch16_224\",\n",
    "                 pretrained: bool = True,\n",
    "                 freeze_backbone: bool = False):\n",
    "        \"\"\"\n",
    "        If timm is available, use timm.create_model(backbone_name, pretrained=True, num_classes=num_classes).\n",
    "        Otherwise try torchvision's vit_b_16 (if installed).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.backbone_name = backbone_name\n",
    "        self.pretrained = pretrained\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "\n",
    "        # Build model\n",
    "        if _USE_TIMM:\n",
    "            # timm handles classifier creation\n",
    "            self.model = timm.create_model(self.backbone_name, pretrained=self.pretrained, num_classes=self.num_classes)\n",
    "        else:\n",
    "            # fallback to torchvision ViT if available\n",
    "            try:\n",
    "                from torchvision import models as tv_models\n",
    "                vit_builder = getattr(tv_models, \"vit_b_16\", None)\n",
    "                if vit_builder is None:\n",
    "                    raise RuntimeError(\"torchvision ViT not available; please install timm.\")\n",
    "                # torchvision vit builder signatures vary; try to create without classifier then add head\n",
    "                backbone = vit_builder(weights=\"IMAGENET1K_V1\") if hasattr(vit_builder, '__call__') else vit_builder(pretrained=self.pretrained)\n",
    "                # remove existing head if present\n",
    "                if hasattr(backbone, 'heads'):\n",
    "                    feat_dim = backbone.heads.head.in_features if hasattr(backbone.heads, 'head') else getattr(backbone, 'hidden_dim', 768)\n",
    "                    backbone.heads = nn.Identity()\n",
    "                elif hasattr(backbone, 'head'):\n",
    "                    feat_dim = backbone.head.in_features\n",
    "                    backbone.head = nn.Identity()\n",
    "                else:\n",
    "                    feat_dim = getattr(backbone, 'hidden_dim', 768)\n",
    "                # create classifier\n",
    "                head = nn.Linear(feat_dim, self.num_classes)\n",
    "                self.model = nn.Sequential(backbone, head)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"No ViT backbone available. Install timm or use recent torchvision.\") from e\n",
    "\n",
    "        # optionally freeze backbone parameters for fine-tuning head only\n",
    "        if self.freeze_backbone:\n",
    "            for name, p in self.model.named_parameters():\n",
    "                if \"head\" not in name and \"heads\" not in name and \"classifier\" not in name:\n",
    "                    p.requires_grad = False\n",
    "\n",
    "        # loss + metrics\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # use torchmetrics for metrics if available\n",
    "        try:\n",
    "            import torchmetrics\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        except Exception:\n",
    "            # fallback simple trackers\n",
    "            self.train_acc = None\n",
    "            self.val_acc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # log loss\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=False)\n",
    "        if self.train_acc is not None:\n",
    "            acc = self.train_acc(preds, y)\n",
    "            self.log(\"train/acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        else:\n",
    "            # rough acc\n",
    "            acc = (preds == y).float().mean()\n",
    "            self.log(\"train/acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        if self.val_acc is not None:\n",
    "            acc = self.val_acc(preds, y)\n",
    "            self.log(\"val/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        else:\n",
    "            acc = (preds == y).float().mean()\n",
    "            self.log(\"val/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True)\n",
    "        if self.val_acc is not None:\n",
    "            acc = self.val_acc(preds, y)\n",
    "            self.log(\"test/acc\", acc, on_step=False, on_epoch=True)\n",
    "        else:\n",
    "            acc = (preds == y).float().mean()\n",
    "            self.log(\"test/acc\", acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        # small scheduler example (cosine)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val/loss\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7388e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Callback lưu loss/acc mỗi epoch để sau đó vẽ\n",
    "class LossHistory(pl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "\n",
    "    def _get_metric(self, trainer, keys):\n",
    "        \"\"\"Trả về float hoặc None — keys là list các key thử (khác PL versions có key khác nhau).\"\"\"\n",
    "        for k in keys:\n",
    "            if k in trainer.callback_metrics:\n",
    "                v = trainer.callback_metrics[k]\n",
    "                try:\n",
    "                    return float(v)\n",
    "                except Exception:\n",
    "                    return float(v.item())\n",
    "        return None\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Thường trainer.callback_metrics sẽ chứa \"train/loss\" (on_epoch=True) và \"val/loss\"\n",
    "        tr_loss = self._get_metric(trainer, [\"train/loss\", \"train_loss\", \"train/loss_epoch\", \"train_loss_epoch\"])\n",
    "        v_loss = self._get_metric(trainer, [\"val/loss\", \"val_loss\", \"val/loss_epoch\", \"val_loss_epoch\"])\n",
    "        v_acc  = self._get_metric(trainer, [\"val/acc\", \"val_acc\", \"val/acc_epoch\", \"val_acc_epoch\"])\n",
    "\n",
    "        # Append only when available (safety)\n",
    "        if tr_loss is not None:\n",
    "            self.train_losses.append(tr_loss)\n",
    "        if v_loss is not None:\n",
    "            self.val_losses.append(v_loss)\n",
    "        if v_acc is not None:\n",
    "            self.val_accs.append(v_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | VisionTransformer  | 85.8 M | train\n",
      "1 | criterion | CrossEntropyLoss   | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.339   Total estimated model params size (MB)\n",
      "279       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=79, pipe_handle=96)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mCan't get attribute 'PlantDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Thay đường dẫn dataset của bạn ở đây\n",
    "data_dir = \"/Users/braly/Desktop/lmvh/plant-identify/dataset\"\n",
    "\n",
    "# hyperparams\n",
    "image_size = 224\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "max_epochs = 10   # chỉnh tuỳ ý\n",
    "precision = 16 if torch.cuda.is_available() else 32\n",
    "gpus = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "# Nếu bạn đã có PlantDataModule và ViTLightning trong notebook, dùng trực tiếp:\n",
    "dm = PlantDataModule(root_dir=data_dir,\n",
    "                     image_size=image_size,\n",
    "                     batch_size=batch_size,\n",
    "                     num_workers=num_workers)\n",
    "dm.setup('fit')\n",
    "print(\"Num classes:\", dm.num_classes)\n",
    "\n",
    "model = ViTLightning(num_classes=dm.num_classes,\n",
    "                     lr=3e-4,\n",
    "                     weight_decay=1e-2,\n",
    "                     backbone_name=\"vit_base_patch16_224\",\n",
    "                     pretrained=True,\n",
    "                     freeze_backbone=False)\n",
    "\n",
    "# Logger + callbacks\n",
    "logger = TensorBoardLogger(save_dir=\"lightning_logs\", name=\"vit_plants_notebook\")\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    monitor=\"val/acc\",     # nếu bạn muốn monitor val/loss thay \"val/acc\"\n",
    "    mode=\"max\",\n",
    "    save_top_k=3,\n",
    "    filename=\"vit-{epoch:02d}-{val/acc:.4f}\"\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "loss_hist_cb = LossHistory()\n",
    "devices_for_trainer = gpus if gpus > 0 else 1\n",
    "# Trainer (an toàn trong notebook: devices=None nếu no GPU)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\" if gpus > 0 else \"cpu\",\n",
    "    devices=gpus if gpus > 0 else 1,  # <-- sửa None thành 1\n",
    "    precision=precision,\n",
    "    callbacks=[checkpoint_cb, lr_monitor, loss_hist_cb],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=50,\n",
    "    enable_progress_bar=True,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model, datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu checkpoint (trainer đã tự lưu theo callback ModelCheckpoint).\n",
    "print(\"Best checkpoint path:\", checkpoint_cb.best_model_path)\n",
    "\n",
    "# Lưu state_dict final của LightningModule (dùng để load bằng model.load_state_dict)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "final_path = \"saved_models/vit_plants_final_state_dict.pth\"\n",
    "torch.save(model.state_dict(), final_path)\n",
    "print(\"Saved state_dict to:\", final_path)\n",
    "\n",
    "# Nếu muốn lưu toàn bộ checkpoint (bao gồm optimizer state) dùng:\n",
    "trainer.save_checkpoint(\"saved_models/vit_plants_full_checkpoint.ckpt\")\n",
    "print(\"Saved full checkpoint to saved_models/vit_plants_full_checkpoint.ckpt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
